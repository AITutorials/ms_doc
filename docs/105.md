




### 任务简述

* 在当前任务中，我们会更深一步的理解项目，分析出针对该项目的类型的独有优化方案：CLS权重分配技术，并且会对测试集的badcase进行分析，从而进行回译数据增强，并实现这些过程。



### 任务目的

* 通过CLS权重分配技术以及badcase分析后的回译数据增强，来修改模型结构并对指定case进行回译数据增强来提升指标。



### 任务步骤
	
* Step1: CLS权重分配技术原理解析与实现
* Step2: CLS权重分配技术应用与效果验证
* Step3: 回译数据增强原理解析与实现
* Step4: 回译数据增强应用与效果验证

---





#### Step1: CLS权重分配技术原理解析与实现

* 当前步骤简述：
	* 在这一步，我们将需要了解CLS权重分配技术起源，原理，实现。

* CLS权重分配技术起源：
	* 在原生BERT预训练过程中，有一个NSP任务，它以“句子对”为输入，判断两句话是否为上下文（二分类）。在当前的项目中，我们使用BERT进行fine-tuning时，也会使用类似的模式。
	* 我们通过之前的学习，知道我们的fine-tuning网路就是1层的全连接，这个网络的输入是我们研究的重点，原生BERT中我们使用标识符[CLS]位置的张量作为输入，把它看作“句子对”的张量。
	* 为什么是[CLS]？因为它在句子的首个位置，不会受token位置的变化的影响，同时，它又不附带任何含义，能够对每个token的语义“平衡”采用。
	* 而CLS权重分配是在另外一种“不平衡”的情况下，因为不是所有的项目需求都希望“平衡”的采用token语义，有时我们可能希望模型更关注某些token或“句子对”中的某一句。这时CLS（微调网络的输入）并不适用[CLS]位置的张量。就需要CLS的权重进行分配，我们往往使用token对应的张量加权来表示。



* CLS权重分配技术原理：
	* 在我们的项目中，我们主要判断[当前句]的情感状态，只是把[当前句的上一句]当作是一种参考，两句话的权重应该是不同的，因此在形成fine-tuning前，这个权重信息应该在“句子对”张量中体现。
	* 我们认为每句话中的token对应张量的加权平均代表该句的张量，此时权重相同的“句子对”张量为：（（句子1每个token张量相加）/ （句子1 token总数） + 句子2每个token张量相加）/ （句子2 token总数））* 0.5。
	* 那么，如果我们认为句子1和句子2权重不同，就可以使用不同的权重来计算，如句子1权重为0.2，句子2权重为0.8，那么句子对张量为：0.2*（句子1每个token张量相加）/ （句子1 token总数）+ 0.8 * 句子2每个token张量相加）/ （句子2 token总数）。


* CLS权重分配技术实现：
	* 首先，我们认为CLS权重分配是替代原生BERT获取[CLS]位置张量的一种方式，因此它是获取BERT特征编码的输出，因此定义函数get_output()
	* 这个函数有三个参数：
		* 1，一个batch_size的输入，其中包含batch_size个token张量组成（这是BERT在进入微调网络前产生的输出）
		* 2，区分token属于哪个句子的token_type_ids（这个是BERT的输入之一）
		* 3，我们希望给更重要句子分配的权重weight
	* 这里认为句子的张量表达为每个token的加性平均，实现可以使用平均池化的方法


* 代码实现位置：
	* /data/sentiment_project/weight_pooled_output.py



#### 让我们动手做起来吧！


* 代码实现：

```python
import torch
import torch.nn as nn

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


def _get_slices(token_type_ids):
    """由token_type_ids获取切片组，利用切片组来区分batch_size中每一组张量哪个是句子1和句子2"""
    slices_list = []
    for slices in token_type_ids:
        # 因为token_type_ids的原生类型是tenor，但是我们需要用到list的一些方法，所以这里转换成list
        slices_ = list(map(lambda x: x.tolist(), slices))
        # token_type_ids 中只有0/1, 第一个1出现的索引就是句子1和句子2的分界点 
        start_index = slices_.index(1)
        # 我们通过1的数量以及start_index就可以获取end_index
        # 这里注意：end_index并不是len(slices)，因为截断补齐时会在后面添加若干个0
        slices_.remove(0)
        end_index = start_index + len(slices_)
        slices_list.append([start_index, end_index])
    return slices_list


def _get_sentence_tensor(T):
    """通过token tensor获取句子张量"""
    mT = nn.AvgPool2d((len(T), 1))
    return mT(T.unsqueeze(0))[0]


def get_output(sample, token_type_ids, weight=0.8):
    output = []
    slices = _get_slices(token_type_ids)
    for i, batch in enumerate(sample):
        A = batch[0 : slices[i][0]]
        B = batch[slices[i][0] : slices[i][1]]
        # 进行平均池化
        A = _get_sentence_tensor(A)
        B = _get_sentence_tensor(B)
        C = weight * B + (1 - weight) * A
        output.append(C.tolist())
    return torch.tensor(output).squeeze(1).to(device)

```



> * 运行示例：

```python
if __name__ == "__main__":
    sample = torch.rand(8, 70, 768)
    # 假设每组句子对的句子都是5和4个token组成
    token_type_ids = torch.tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1]] * 8)
    res = get_output(sample, token_type_ids, weight=0.8)
    print(res)
    print(res.shape)
```


```text
tensor([[0.4659, 0.5024, 0.5184,  ..., 0.5912, 0.5129, 0.6166],
        [0.4729, 0.4052, 0.4818,  ..., 0.5221, 0.4641, 0.5344],
        [0.5687, 0.4602, 0.6082,  ..., 0.3549, 0.6270, 0.4734],
        ...,
        [0.5721, 0.4761, 0.4805,  ..., 0.5311, 0.3121, 0.5416],
        [0.5511, 0.4343, 0.5633,  ..., 0.6058, 0.4898, 0.4959],
        [0.4219, 0.5421, 0.4685,  ..., 0.7422, 0.3502, 0.2999]],
       device='cuda:0')
torch.Size([8, 768])
```

* 当前步骤总结：
	* 通过这一步，我们了解了有关CLS权重分配技术的原理和实现，这是该项目独有的优化思路，当然也适用具有同等场景的项目，我们接下来将在训练代码使用它。


---


#### Step2: CLS权重分配技术应用与效果验证

* 当前步骤简述：
	* 在这一步，我们将把之前实现的CLS权重分配方法应用在项目之中，并进行实验来获取它的效果。


* 使用说明：
        * 在源码中添加函数，我们往往会将其封装成pypi工具包，防止需要管理过多的依赖文件，在这里，我们已经其封装在pyaitools1.4.3之中，所以我们直接```pip install pyaitools==1.4.3```, 然后在源码中
直接使用。


> * 使用方法：

```python
from pyaitools.weight_pooled_output import get_output
pooled_output = get_output(outputs[0], token_type_ids, weight=0.9)
```



* 修改说明：
	* 我们会找到BERT源码中原生取CLS的位置，将其注释，换成通过权重分配获取的新的pooled_output。


* 代码实现位置：
	* /root/anaconda3/envs/ms3.7/lib/python3.7/site-packages/transformers/modeling_bert.py



#### 让我们动手做起来吧！


* 代码实现：

```python hl_lines="10 11"
1169         outputs = self.bert(
1170             input_ids,
1171             attention_mask=attention_mask,
1172             token_type_ids=token_type_ids,
1173             position_ids=position_ids,
1174             head_mask=head_mask,
1175             inputs_embeds=inputs_embeds,
1176         )
1177
1178         from pyaitools.weight_pooled_output import get_output
1179         pooled_output = get_output(outputs[0], token_type_ids, weight=0.9)
1180         # pooled_output = outputs[1]
1181
1182         pooled_output = self.dropout(pooled_output)
1183         logits = self.classifier(pooled_output)
1184
1185         outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here
1186
1187         if labels is not None:
1188             if self.num_labels == 1:
1189                 #  We are doing regression
1190                 loss_fct = MSELoss()
1191                 loss = loss_fct(logits.view(-1), labels.view(-1))
```

> * 重新训练：

```python
sh start_train.sh
```



* 权重分配的效果：

<center>

|weght|准确率|
|--|--|
|0.6|91.5%|
|0.7|92.1%|
|0.8|92.7%|
|0.9|92.7%|

</center>

---

* 结果分析：
	* 在当前的数据上，使得模型更加关注句子2，或者说更关注来自句子2的特征，能够帮助更好的提升泛化，说明句子2的特征更加具有区分度（打标签的人员主要通过句子2来判断情感），当权重足够大时，效果趋于稳定。




* 当前步骤总结：
	* 在这一步，我们将CLS权重分配技术应用在模型训练过程中，并获得一定的优化效果，同学们可以基于此进行更多尝试。




---

#### Step3: 回译数据增强原理解析与实现

* 当前步骤简述：
	* 在这一步，我们将实现文本数据增强中一种重要方法---回译增强法，它的原理，适用条件，使用要求以及实现都会在接下来详细说明。



* 回译数据增强原理：
	* 这是一种借助翻译工具的方法，通过不同语言之间对“同一表述”可能存在的差异来增强数据，我们会将原始样本翻译成另外一种语言，甚至再连续翻译成多种语言，然后再把他们翻译回来，大多数情况下，你的原始样本表达方式会有一些变化，正是这种不改变语义（不改变标签）的变化可能带给模型更多的能力。但与此同时，这种方法也受到翻译工具能力的影响。（但一般认为，翻译工具虽然有预测瑕疵，但是不会影响基本语义）


* 适用条件：
	* 通过上面的原理可知，回译数据增强受到工具的影响，往往会出现语言瑕疵（错词，语法问题等），所以适用于闲聊语料，不适合于专业领域语料。而我们的直播间场景对话就属于闲聊语料，语言瑕疵不会带来很大的影响（正常人打字闲聊也可能出现错词等）。


* 使用要求：
	* 回译数据增强往往和badcase分析（分析验证集/测试集中模型预测错误的样本）结合使用，在我们做完badcase分析之后，找到所有错误的样本，为了增强模型对这部分样本的泛化，会采用回译数据增强；
	* 这种方式能够有效减少回译数据增强本身的不规范性带给正样本的冲击，避免误差引入；同时，导致错误样本识别不正确的原因，往往是因为该类型的样本较少或语义较复杂，当通过增强来添加数量规模时，往往取得更好的效果。



* 回译数据增强的实现:
	* 首先就是找到一款翻译工具，这里使用pygtrans（需要外网访问），```pip install pygtrans```。
	* 我们以验证集dev.tsv为输入，对其中的文本进行数据增强，这里会使用pandas来读取数据。
	* 因为我们现在例子是英文作为起始语言，因此需要最终也翻译回英文，翻译链条为：韩语，中文，日语，英文（['ko', 'zh-cn', 'ja', 'en']）。



* 代码实现位置：
	* /data/sentiment_project/data/data_augmentation.py


#### 让我们动手做起来吧!

* 代码实现：

```python
from pygtrans import Translate

def back_translation(input_, language_list=['ko', 'zh-cn', 'ja', 'en']):
    """完成回译增强"""
    client = Translate()
    for lg in language_list:
        texts = client.translate(input_, target=lg)
        input_  = list(map(lambda x: x.translatedText, texts))
    return input_
```

> * 运行示例：

```python
import pandas as pd

# 读取验证集
df = pd.read_csv("dev.tsv", sep="\t")

# 将验证集中的标签和文本对分别取出来
labels = []
data = []
for _, s1, s2, label in df.values:
    labels.append(label)
    data.append({"sentence1": s1, "sentence2": s2})

# 调用回译增强
s1_list = list(map(lambda x: x["sentence1"], data))
s2_list = list(map(lambda x: x["sentence2"], data))

s1_trans_list = back_translation(s1_list)
s2_trans_list = back_translation(s2_list)
print(s1_trans_list)
print(s2_trans_list)
```

```text
['He is the luckiest child in the world.', 'Oi.', 'This is Monica. I&#39;m very tired of stalking guys and keeping their panties.', 'are you OK. Wait, wait, why don&#39;t you come with me?', 'It would be very helpful if you could make me look beautiful here.', 'Suicide more!', 'please look.', 'all right. But don&#39;t worry. Cereals, muffins, waffles,', 'all right. Take your jacket and take a taxi.', 'God, he lost. He lost it altogether.', 'Hello Kelly, how are you?', 'know!', 'What did you just do', 'But don&#39;t think you can&#39;t visit.', 'If not, I will.', 'No, wait!', 'Yes, well, you may know that you need to work a little more!', 'Do you know what&#39;s wrong?', 'Yes! Yes! Yes! I seem to be doing something exciting, but I don&#39;t want to get it', 'Did you know that sometimes great ideas are right in front of you?', 'of course! This is my best friend Joey.', 'Real? !!', 'I still don&#39;t understand it, we didn&#39;t do anything wrong.', 'They are', 'Currently, there are herbal teas that you can drink.', 'Who are you, my god? !!', 'You see, Monica', 'Chandler is a woman!', 'Do you know these cute guys?', 'Everyone, I really hope this guy likes me.', 'See you later.', 'No, don&#39;t turn it off. Because ... the key is ... inside.', 'Besides, nothing works in Bing.', 'Yeah, that&#39;s why I&#39;m not that big', 'The ship set sail.', 'To do. Yes you love her You always have a baby. There is no correct answer.', 'Proposal at planetarium', 'Real? !!', 'Oh! Did you think you got married in January?', 'all right. Let&#39;s start.', 'OK, this time I&#39;m going back to the bucket.', 'are you OK.', 'Yes, it&#39;s a little sweet, hmm ...', 'Did you just make', 'So are we not going to live together anymore?', 'Real? !!', 'No, it doesn&#39;t matter. Anyone with 30 fingers and 30 toes is healthy.', '..Excuse me?', 'Hmm ... maybe we ...', 'look!', 'I have to talk to you!', 'Do you really think so?', 'Yes. Yes, it is. Just below. There, yes, why can I ask?', 'No, no, he wasn&#39;t around.', 'do not do! why?', 'I got another one!', 'The number of sandwiches you can eat on the floor is limited.', 'Yes! Yes i did', 'You see, I think Elton John wrote for him', 'Oh! here! Oh Joey! Why did you sign &quot;You are a bastard&quot;?', 'know.', 'Well, we are ready to try anything.', 'As you can see, this is not possible.', 'Hey, little Joey is dead, I have no reason to live!', 'Um, what?']
```



* 当前步骤总结：
	* 到这里，我们完成了回译数据增强的实现，在实际使用中，我们还会有很多注意事项，比如不会对所有的验证集/训练集/测试集内容进行数据增强，对于增强后的内容一般也要进行一些基本的筛查，我们将在后面的步骤中完善它。





---

#### Step4: 回译数据增强应用与效果验证

* 当前步骤简述：
	* 在这一步，我们将之前的回译数据增强应用在项目之中，并验证它带来的效果。


* Badcase分析：
	* 回译数据增强往往和Badcase分析结合使用，所谓Badcase分析，一般是通过训练好的模型对验证集进行预测，然后收集和分析那些错误例子的过程。
	


* Badcase分析实现说明：
	* 实现中可以借助当前的在线模型服务，对验证集进行预测，然后筛选出标签不一致的样本。
	* 当然，也可以选择离线模型直接预测，这都是可行的。
	* 实现位置：/data/sentiment_project/data

* 回译数据增强应用：
	* 首先，我们会实现Badcase分析，通过请求模型服务来预测样本
	* 然后对错误的样本进行回译数据增强，并进行一定的过滤，保证增强后的数据与验证集原始数据一定不同
	* 最后将这些数据归入到训练集中重新训练，再进行效果对比
	* 实现位置：/data/sentiment_project/data/data_augmentation.py (在之前代码实现的基础上做一些改进)



### 让我们动手做起来吧!

* 代码实现：






