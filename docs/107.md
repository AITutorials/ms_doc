




### 任务简述



* 上一个任务我们实现了Resnet+GRU，可这样传统的模型组合在实际效果上总是很难让人满意。因此，在这个任务中我们将选择更具有竞争力的模型：构建的VisualBERT，相当于R+G模型的升级，它使用预训练+微调的范式解决实际问题，从数据，模型结构，工具使用多个角维度进行工业实践。





### 任务目的

* 构建基于VisualBERT多模态模型，完成该模型的训练流程。


### 任务步骤
	
* Step1: 多模态模型VisualBERT介绍与构建
* Step2: VisualBERT的训练实现
* Step3: 使用co-attention对模态融合源码进行修改

---



#### Step1: 多模态模型VisualBERT介绍与构建

* 多模态模型VisualBERT介绍
	* 随着迁移学习技术越来越成熟，数据积累越来越多，大型模型在多模态领域的表现越来越抢眼。以VisualBERT为代表的单流模型是时代之星，尤其在拥有海量数据的工业界，VisualBERT应用更是广泛。下面我们将简单介绍该模型，并实现对其的进一步优化。


* VisualBERT结构示意图：

![](http://www.tisv.cn:8900/img/visualBERT.png)

* 模型的主要结构为借助一组堆叠的transformer encoder层，通过自注意力机制把把输入一段文本中的元素和一张相关的输入图像中的区域隐式地对⻬起来。如下图所示，其中图像通过检测的方法提取目标 region作为基础的视觉单元，与文本区域进行拼接后作为transformer的输入。
> * 注：需要对Transformer有一定的基础。


* 代码实现位置：
	* 不存在的路径需要自己的创建
	* /data/labeled_project/multimodal_labeled/model_train/visualBERT_train.py


#### 让我们动手做起来吧！


* 代码实现：

```python

import torch
import torch.nn as nn
import numpy as np
import torchvision
from torchvision import datasets, models, transforms

from torchvision import datasets, models, transforms
from transformers import BertTokenizer, VisualBertForQuestionAnswering, VisualBertConfig

### 模型构建与实验

class VisualBERTModel(nn.Module):
    def __init__(
        self,
    ):
        super(VisualBERTModel, self).__init__()
        self.model = VisualBertForQuestionAnswering.from_pretrained("uclanlp/visualbert-vqa-coco-pre")
        self.model.config.max_position_embeddings = 200
        self.tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
        #self.model = VisualBertForQuestionAnswering(self.config)


    def forward(self, text, visual_embeds):
        # padding=True进行长度向后补齐
        inputs = self.tokenizer(text, return_tensors="pt", padding=True)
        visual_embeds = visual_embeds.view(visual_embeds.size(0), -1, 2048)
        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)
        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)
        inputs.update({
              "visual_embeds": visual_embeds,
              "visual_token_type_ids": visual_token_type_ids,
              "visual_attention_mask": visual_attention_mask
        })
        outputs = self.model(**inputs)
        return outputs.logits
```


> * 运行示例：

```python
model = VisualBERTModel()
# 随机单条数据
visual_embeds = torch.rand(1, 3, 1280, 720)
text = "不要再想你"
print(model(text, visual_embeds))
```



```text
tensor([[ 0.1722, -0.2820]], grad_fn=<ViewBackward>)
```

* 当前步骤总结：
	* 通过这一步，我们借助huggingface工具构建了ViusalBERT模型，并跑通了它的forward过程。


---


#### Step2: VisualBERT的训练实现 

* 当前步骤简述：
	* 在这里我们将会实现VisualBERT的训练过程，它和之前的R+G模型训练类似。




* 代码实现位置：
	* /data/labeled_project/multimodal_labeled/model_train/visualBERT_train.py



#### 让我们动手做起来吧！


* 代码实现：

```python
import time
import os
import copy


# 读取数据并转成json
path = "dev_sent_emo.jsonl"
with open(path, 'r') as fr:
    multimodal_data_list = fr.readlines()

# 处理文本读取时附带一些符号，比如换行符
multimodal_data_list = list(map(lambda x: eval(x[:-1]), multimodal_data_list))


train_txt_list = list(map(lambda x: x["text"], multimodal_data_list))

### 图片预处理
# 根据之前的数据分析获得
gold_size = (1280, 720)
# 使模型输入张量服从标准正态分布，第一个参数为均值列表，代表各个通道的均值，
# 第二个参数为标准差列表，代表各个通道的标准差。这里的图片都是有三个通道。
# 其中均值和标准差列表中的数值来自对ImageNet的全局采样结果。
gold_normalize = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])

# 定义一系列标准处理流程，Resize，张量化，规范化
# Resize和张量化用于统一图片尺寸和满足框架要求
# 规范化便于模型快速收敛
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(gold_size),
        transforms.ToTensor(),
        transforms.Normalize(gold_normalize[0], gold_normalize[1])
    ]),
    'val': transforms.Compose([
        transforms.Resize(gold_size),
        transforms.ToTensor(),
        transforms.Normalize(gold_normalize[0], gold_normalize[1])
    ]),
}

train_data_dir_list = list(map(lambda x: x["img"], multimodal_data_list))

from PIL import Image

def img_loader(path):
    with open(path, 'rb') as f:
        img = Image.open(f)
        return img.convert('RGB')


train_img_list = list(map(lambda x: data_transforms["train"](img_loader(x)), train_data_dir_list))


### 获取标签

train_label_list = list(map(lambda x: x["label"], multimodal_data_list))

print(train_label_list)
### 整合数据成为DataLoader
from torch.utils.data import DataLoader

train_iter = list(zip(train_img_list, train_txt_list, train_label_list))

BATCH_SIZE = 8

train_dataloader = DataLoader(
    train_iter, batch_size=BATCH_SIZE, shuffle=True
)



### 模型训练


import time

def train(dataloader):
    model.train()
    total_acc, total_count = 0, 0
    log_interval = 500
    start_time = time.time()
    for idx, (img, txt, label) in enumerate(dataloader):
        optimizer.zero_grad()
        predited_label = model(txt, img)
        loss = criterion(predited_label, label)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
        optimizer.step()
        total_acc += (predited_label.argmax(1) == label).sum().item()
        total_count += label.size(0)
        if idx % log_interval == 0 and idx > 0:
            elapsed = time.time() - start_time
            print(
                "| epoch {:3d} | {:5d}/{:5d} batches "
                "| accuracy {:8.3f}".format(
                    epoch, idx, len(dataloader), total_acc / total_count
                )
            )
            total_acc, total_count = 0, 0
            start_time = time.time()


def evaluate(dataloader):
    model.eval()
    total_acc, total_count = 0, 0

    with torch.no_grad():
        for idx, (img, txt, label) in enumerate(dataloader):
            predited_label = model(txt, img)
            loss = criterion(predited_label, label)
            total_acc += (predited_label.argmax(1) == label).sum().item()
            total_count += label.size(0)
    return total_acc / total_count


# Hyperparameters
EPOCHS = 20  # epoch
LR = 0.05  # learning rate

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 0.99, gamma=0.1)
total_accu = None
test_dataloader = valid_dataloader = train_dataloader

for epoch in range(1, EPOCHS + 1):
    epoch_start_time = time.time()
    train(train_dataloader)
    accu_val = evaluate(valid_dataloader)
    if total_accu is not None and total_accu > accu_val:
        scheduler.step()
    else:
        total_accu = accu_val
    print("-" * 59)
    print(
        "| end of epoch {:3d} | time: {:5.2f}s | "
        "valid accuracy {:8.3f} ".format(
            epoch, time.time() - epoch_start_time, accu_val
        )
    )
    print("-" * 59)


print("Checking the results of test dataset.")
accu_test = evaluate(test_dataloader)
print("test accuracy {:8.3f}".format(accu_test))

torch.save(model.state_dict(), './model_weights.pth')
```

> * 运行效果：

```python

```


* 当前步骤总结：
	* 通过这一步，我们对visualBERT进行了基线训练，工业实践中效果明显优于R+G模型，之后我们将在这个模型上进行一些列优化。




---

#### Step3: 使用co-attention对模态融合源码进行修改

* 当前步骤简述：
	* 在这一步我们将对原生的visualBERT进行结构优化，针对模态融合的方式进行改动，将原有的运算变成流行的co-attention方法。。



* 什么是co-attention：
	* 来自论文Hierarchical Question-Image Co-Attention for Visual Question Answering)，其为模型的核心部分，相比单纯的自注意力机制，互注意力在进行运算时交换了两种模态self-attention运算时的K/V查询对，让两种模态的进行充分的学习，是模态融合的重要方法。


![](http://www.tisv.cn:8900/img/vilbert2.png)



* 需要完成的内容：
	* 实现co-attention机制
	* 在源码中找到embedding融合的位置，并将co-attention加入

* 代码实现位置：
	* 实现co-attention的位置：/data/labeled_project/multimodal_labeled/model_train/co_attention.py 
	* 修改源码的路径： /root/anaconda3/envs/multimodal3.8/lib/python3.8/site-packages/transformers/models/visual_bert/modeling_visual_bert.py 

#### 让我们动手做起来吧!

* co-attentio代码实现：

```python
import torch
import torch.nn as nn
import math


class Config:
    def __init__(self):
        super().__init__()
        self.bi_num_attention_heads = 12
        self.bi_hidden_size = 768
        self.v_hidden_size = 768
        self.hidden_size = 768
        self.v_attention_probs_dropout_prob = 0.2
        self.attention_probs_dropout_prob = 0.2


# 互自注意力子层
class BertBiAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        # 多头注意力机制中多头的数量
        self.num_attention_heads = config.bi_num_attention_heads

        # 每个attention头输出的张量最后一维的尺寸
        # 其中config.bi_hidden_size是指希望通过bi-attention之后输出的张量最后一维的尺寸
        # 因为最后要做“拼接”操作，因此每个attention头输出的张量尺寸为二者的商
        self.attention_head_size = int(
            config.bi_hidden_size / config.bi_num_attention_heads
        )

        # 非特殊情况下self.all_head_size与config.bi_hidden_size是相同的
        # 不过二者应用的含义不同，self.all_head_size是指QKV全连接层的输出维度
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        # 视觉Q/K/V参数矩阵
        # 注：按照自注意力机制的原理，一般QKV参数矩阵都是方阵，
        # 即config.v_hidden_size = self.all_head_size
        self.query1 = nn.Linear(config.v_hidden_size, self.all_head_size)
        self.key1 = nn.Linear(config.v_hidden_size, self.all_head_size)
        self.value1 = nn.Linear(config.v_hidden_size, self.all_head_size)
        self.dropout1 = nn.Dropout(config.v_attention_probs_dropout_prob)

        # 文本Q/K/V参数矩阵
        self.query2 = nn.Linear(config.hidden_size, self.all_head_size)
        self.key2 = nn.Linear(config.hidden_size, self.all_head_size)
        self.value2 = nn.Linear(config.hidden_size, self.all_head_size)
        self.dropout2 = nn.Dropout(config.attention_probs_dropout_prob)

    def transpose_for_scores(self, x):
        """在进入attention计算公式前需要做一些准备
        首先需要view，将QKV全连接输出的Q／K／V按头分割
        然后对第二维和第三维进行转置操作，
        为了让代表句子长度维度和词向量维度能够相邻，这样注意力机制才能找到词义与句子位置的关系，
        从attention函数中可以看到，利用的是原始输入的倒数第一和第二维.这样我们就得到了每个头的输入.
        """

        # x的最后一维变成两维，需保证最后一维的size = self.num_attention_heads * self.attention_head_size
        new_x_shape = x.size()[:-1] + (
            self.num_attention_heads,
            self.attention_head_size,
        )
        # 这样输入的三维张量变成了四维，从文本角度，第二为代表长度应该与最后一维的词向量相邻
        x = x.view(new_x_shape)

        # 因此在这里将文本长度维度与attention头数维度调换
        return x.permute(0, 2, 1, 3)

    def forward(self, input_tensor1, input_tensor2):
        # 对视觉输入计算向量
        mixed_query_layer1 = self.query1(input_tensor1)
        mixed_key_layer1 = self.key1(input_tensor1)
        mixed_value_layer1 = self.value1(input_tensor1)
        query_layer1 = self.transpose_for_scores(mixed_query_layer1)
        key_layer1 = self.transpose_for_scores(mixed_key_layer1)
        value_layer1 = self.transpose_for_scores(mixed_value_layer1)
        # 对文本输入计算向量
        mixed_query_layer2 = self.query2(input_tensor2)
        mixed_key_layer2 = self.key2(input_tensor2)
        mixed_value_layer2 = self.value2(input_tensor2)
        query_layer2 = self.transpose_for_scores(mixed_query_layer2)
        key_layer2 = self.transpose_for_scores(mixed_key_layer2)
        value_layer2 = self.transpose_for_scores(mixed_value_layer2)
        # attention scores for value 1. 这是关键部分，主要为计算text query 和 image 的 key的结果
        # 视觉的query 与文本的Key的转置进行交叉
        attention_scores1 = torch.matmul(query_layer2, key_layer1.transpose(-1, -2))
        attention_scores1 = attention_scores1 / math.sqrt(self.attention_head_size)
        # 通过softmax 归一化得分概率
        attention_probs1 = nn.functional.softmax(attention_scores1, dim=-1)
        attention_probs1 = self.dropout1(attention_probs1)
        # 得到的归一化概率与文本value进行运算得到context_layer
        context_layer1 = torch.matmul(attention_probs1, value_layer1)

        # 变换成原来的维度以及对应的含义
        context_layer1 = context_layer1.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape1 = context_layer1.size()[:-2] + (self.all_head_size,)
        context_layer1 = context_layer1.view(new_context_layer_shape1)
        # 文本的query与视觉的key 进行运算，运算方式与上面相同
        attention_scores2 = torch.matmul(query_layer1, key_layer2.transpose(-1, -2))
        attention_scores2 = attention_scores2 / math.sqrt(self.attention_head_size)
        # Normalize
        attention_probs2 = nn.functional.softmax(attention_scores2, dim=-1)
        attention_probs2 = self.dropout2(attention_probs2)
        context_layer2 = torch.matmul(attention_probs2, value_layer2)

        # 变换成原来的维度以及对应的含义
        context_layer2 = context_layer2.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape2 = context_layer2.size()[:-2] + (self.all_head_size,)
        context_layer2 = context_layer2.view(new_context_layer_shape2)
        attn_data = {
            "attn1": attention_probs1,
            "queries1": query_layer2,
            "keys1": key_layer1,
            "attn2": attention_probs2,
            "querues2": query_layer1,
            "keys2": key_layer2,
        }
        # context_layer1, context_layer2 is for text and vision
        return context_layer1, context_layer2, attn_data

```


> * 运行示例:

```python
if __name__ == "__main__":
    config = Config()
    biattention = BertBiAttention(config)
    input_tensor1 = torch.rand(3, 8, 768)
    input_tensor2 = torch.rand(3, 525, 768)
    context_layer1, context_layer2, attn_data = biattention(
        input_tensor1, input_tensor2
    )
    print(context_layer1, context_layer1.shape)
    print(context_layer2, context_layer2.shape)

```


```text
tensor([[[-9.6074e-02, -8.1292e-02, -1.7113e-01,  ...,  1.8071e-01,
           3.1835e-01,  7.8117e-02],
         [-9.4354e-02, -8.3363e-02, -1.5782e-01,  ...,  2.0677e-01,
           3.5806e-01,  1.2813e-01],
         [-6.2287e-02,  2.4098e-04, -1.2370e-01,  ...,  1.9457e-01,
           3.7959e-01,  1.0256e-01],
         ...,
         [-1.1077e-01, -3.9221e-02, -1.8073e-01,  ...,  2.3376e-01,
           3.9866e-01,  1.1799e-01],
         [-1.1786e-01, -2.9357e-02, -1.9034e-01,  ...,  2.3433e-01,
           3.9668e-01,  1.1613e-01],
         [-1.1233e-01,  1.0437e-03, -9.6470e-02,  ...,  2.3463e-01,
           3.9997e-01,  1.1814e-01]]], grad_fn=<ViewBackward>) torch.Size([3, 8, 516])
tensor([[[-0.2471,  0.0176,  0.0424,  ...,  0.3034, -0.2601, -0.2893],
         [-0.2232,  0.0199,  0.0218,  ...,  0.1760, -0.1642, -0.1626],
         [-0.2953,  0.0326,  0.0608,  ...,  0.2543, -0.2102, -0.2610],
         ...,
         [-0.2469,  0.0377,  0.0494,  ...,  0.3083, -0.2558, -0.2803],
         [-0.2957,  0.0297,  0.0630,  ...,  0.3144, -0.2671, -0.3090],
         [-0.2354,  0.0436,  0.0457,  ...,  0.1931, -0.2148, -0.2196]]],
       grad_fn=<ViewBackward>) torch.Size([3, 525, 516])
```


* 源码修改使其成为VisualBERT中的一部分：
	* 需要将co_attention.py移到源码路径下
```python hl_lines="1 2 18 19 20" 
from .co_attention import BertBiAttention
from .co_attention import Config

class VisualBertEmbeddings(nn.Module):
    """Construct the embeddings from word, position and token_type embeddings and visual embeddings."""

    def __init__(self, config):
        super().__init__()
...
...
            else:
                visual_position_ids = torch.zeros(
                    *visual_embeds.size()[:-1], dtype=torch.long, device=visual_embeds.device
                )
                visual_position_embeddings = self.visual_position_embeddings(visual_position_ids)

            visual_embeddings = visual_embeds + visual_position_embeddings + visual_token_type_embeddings
            config = Config()
            bi_attention = BertBiAttention(config)
            embeddings, visual_embeddings, _ = bi_attention(embeddings, visual_embeddings)
            embeddings = torch.cat((embeddings, visual_embeddings), dim=1)

        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings
```


* 实际效果对比：

<center>

||准确率|
|--|--|
|未使用co-attention|85.5%|
|使用co-attention|90.3%|

</center>


* 当前步骤总结：
	* 基于co-attention的源码修改，我们的指标基线会有5%以上的提升达到90%，这已经能满足标签系统的使用，再之后的任务中，我们将着重讲解对效率的优化。


