




### 任务简述


* 在上一个任务中，我们关注VisualBERT的核心结构改进，针对模态交互部分的代码进行优化，使用了主流的co-attention。在本任务中，我们将更关注模型效率的提升，针对co-attention带来的效率下降，我们将实现更多改进方案。






### 任务目的

* 针对co-attention带来的效率下降，实现更多改进方案，包括分块方法和稀疏化方法。


### 任务步骤

* Step1: co-attention效率优化的常用方案说明
* Step2: 分块思想优化方案
* Step3: 稀疏化思想优化方案

--- 



#### Step1: co-attention效率优化的常用方案说明


* co-attention效率问题：
	* 无论是co-attention还是self-attention都是N方的时间复杂度（矩阵乘法），因此当我们的输入张量较大时，显存的占用和计算性能都是指数增长的；


![](http://www.tisv.cn:8900/img/attention1.png)


* attention的操作优化的四大思想：
	* 分块思想（chunk）: 将原有的token进行分块，这类似于池化，原来NxN变成了BxB，B远小于N；
	* 稀释化思想：以一定的规律对N进行下采样，只留下K个，K远小于N；
	* 数学压缩思想：使用矩阵分解或核函数映射将N减小再做attention；
	* 可学习思想：在之前的思想方法上引入训练参数，在学习中调节控制N的缩小范围。

* 当前步骤总结：
	* 这一步是让大家回顾一下attention的效率问题以及当前业界解决该问题的主要思想，我们会在接下来的步骤中实现分块思想和稀释化思想两种最常见的方案。


---


#### Step2: 分块思想优化方案

* 当前步骤简述：
	* 基于分块思想进行attention效率提升的方案设计。


* 方法简述：
	* 原理：以NLP中的attention为例，将原来单字对单字的一一对应，变成的多个字对多个字的一一对应，块中的所有字的对应分值一致共享。
	* 实现： 我们使用torch中的池化操作来进行，因为池化操作会改变张量维度，因此它将会分为步骤：
		* 进行池化
		* 恢复维度
	* 拓展：关于如何去分块，每个块的表示张量是否只能为均值，这都是可以进行改进的点。


* 代码实现位置：
	* /data/labeled_project/multimodal_labeled/model_train/co_attention_optimization.py



#### 让我们动手做起来吧！


* 代码实现：

```python
import torch
import torch.nn as nn

def maxtri_chunking(Q, chunks=2):
    """
    其中每一个chunk的计算规则可自定义，一般为平均池化
    example:
        Q.shape = (1, 1, 10, 43)
        res.shape = (1, 1, 5, 43)
    """
    # AvgPool2d代表对最后两维进行池化操作，它的输入是四维张量
    # 其中的参数代表池化操作的chunk大小，它是一个元组，
    # 第一个参数代表两“列”相加求平均，第二个1代表1“行”操作即行上数值不变
    m = nn.AvgPool2d((chunks, 1))
    return m(Q)



def maxtri_chunking_restore(Q, chunks=2):
    """
    Q.shape = (5, 5)
    result.shape = (10, 10)

    相当于输入进行上下堆叠
    """
    temp = torch.cat([Q]*chunks, dim=-1)
    return torch.cat([temp]*chunks, dim=-2)
```

> * 运行示例：

```python
if __name__ == "__main__":
    Q = torch.rand(1, 1, 10, 43)
    r = maxtri_chunking(Q)
    print(r.shape)

    e = maxtri_chunking_restore(r)
    print(e.shape)
```

```text
torch.Size([1, 1, 5, 43])
torch.Size([1, 1, 10, 86])
```


* 应用代码实现：
	* 位置：/data/labeled_project/multimodal_labeled/model_train/co_attention_chunks.py

```python
import torch
import torch.nn as nn
import math
from co_attention_optimization import maxtri_chunking as MC
from co_attention_optimization import maxtri_chunking_restore as MCR


class Config:
    def __init__(self):
        super().__init__()
        self.bi_num_attention_heads = 12
        self.bi_hidden_size = 768
        self.v_hidden_size = 768
        self.hidden_size = 768
        self.v_attention_probs_dropout_prob = 0.2
        self.attention_probs_dropout_prob = 0.2


# 互自注意力子层
class BertBiAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        # 多头注意力机制中多头的数量
        self.num_attention_heads = config.bi_num_attention_heads

        # 每个attention头输出的张量最后一维的尺寸
        # 其中config.bi_hidden_size是指希望通过bi-attention之后输出的张量最后一维的尺寸
        # 因为最后要做“拼接”操作，因此每个attention头输出的张量尺寸为二者的商
        self.attention_head_size = int(
            config.bi_hidden_size / config.bi_num_attention_heads
        )

        # 非特殊情况下self.all_head_size与config.bi_hidden_size是相同的
        # 不过二者应用的含义不同，self.all_head_size是指QKV全连接层的输出维度
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        # 视觉Q/K/V参数矩阵
        # 注：按照自注意力机制的原理，一般QKV参数矩阵都是方阵，
        # 即config.v_hidden_size = self.all_head_size
        self.query1 = nn.Linear(config.v_hidden_size, self.all_head_size)
        self.key1 = nn.Linear(config.v_hidden_size, self.all_head_size)
        self.value1 = nn.Linear(config.v_hidden_size, self.all_head_size)
        self.dropout1 = nn.Dropout(config.v_attention_probs_dropout_prob)

        # 文本Q/K/V参数矩阵
        self.query2 = nn.Linear(config.hidden_size, self.all_head_size)
        self.key2 = nn.Linear(config.hidden_size, self.all_head_size)
        self.value2 = nn.Linear(config.hidden_size, self.all_head_size)
        self.dropout2 = nn.Dropout(config.attention_probs_dropout_prob)

    def transpose_for_scores(self, x):
        """在进入attention计算公式前需要做一些准备
        首先需要view，将QKV全连接输出的Q／K／V按头分割
        然后对第二维和第三维进行转置操作，
        为了让代表句子长度维度和词向量维度能够相邻，这样注意力机制才能找到词义与句子位置的关系，
        从attention函数中可以看到，利用的是原始输入的倒数第一和第二维.这样我们就得到了每个头的输入.
        """

        # x的最后一维变成两维，需保证最后一维的size = self.num_attention_heads * self.attention_head_size
        new_x_shape = x.size()[:-1] + (
            self.num_attention_heads,
            self.attention_head_size,
        )
        # 这样输入的三维张量变成了四维，从文本角度，第二为代表长度应该与最后一维的词向量相邻
        x = x.view(new_x_shape)

        # 因此在这里将文本长度维度与attention头数维度调换
        return x.permute(0, 2, 1, 3)

    def forward(self, input_tensor1, input_tensor2):
        # 对视觉输入计算向量
        mixed_query_layer1 = self.query1(input_tensor1)
        mixed_key_layer1 = self.key1(input_tensor1)
        mixed_value_layer1 = self.value1(input_tensor1)
        query_layer1 = self.transpose_for_scores(mixed_query_layer1)
        key_layer1 = self.transpose_for_scores(mixed_key_layer1)
        value_layer1 = self.transpose_for_scores(mixed_value_layer1)
        # 对文本输入计算向量
        mixed_query_layer2 = self.query2(input_tensor2)
        mixed_key_layer2 = self.key2(input_tensor2)
        mixed_value_layer2 = self.value2(input_tensor2)
        query_layer2 = self.transpose_for_scores(mixed_query_layer2)
        key_layer2 = self.transpose_for_scores(mixed_key_layer2)
        value_layer2 = self.transpose_for_scores(mixed_value_layer2)

        query_layer2 = MC(query_layer2)
        key_layer1 = MC(key_layer1)
        query_layer1 = MC(query_layer1)
        key_layer2 = MC(key_layer2)

        # attention scores for value 1. 这是关键部分，主要为计算text query 和 image 的 key的结果
        # 视觉的query 与文本的Key的转置进行交叉
        attention_scores1 = torch.matmul(query_layer2, key_layer1.transpose(-1, -2))
        attention_scores1 = attention_scores1 / math.sqrt(self.attention_head_size)
        # 通过softmax 归一化得分概率
        attention_probs1 = nn.functional.softmax(attention_scores1, dim=-1)
        attention_probs1 = self.dropout1(attention_probs1)
        # 得到的归一化概率与文本value进行运算得到context_layer
        attention_probs1 = MCR(attention_probs1)
        context_layer1 = torch.matmul(attention_probs1, value_layer1)

        # 变换成原来的维度以及对应的含义
        context_layer1 = context_layer1.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape1 = context_layer1.size()[:-2] + (self.all_head_size,)
        context_layer1 = context_layer1.view(new_context_layer_shape1)
        # 文本的query与视觉的key 进行运算，运算方式与上面相同
        attention_scores2 = torch.matmul(query_layer1, key_layer2.transpose(-1, -2))
        attention_scores2 = attention_scores2 / math.sqrt(self.attention_head_size)
        # Normalize
        attention_probs2 = nn.functional.softmax(attention_scores2, dim=-1)
        attention_probs2 = self.dropout2(attention_probs2)
        attention_probs2 = MCR(attention_probs2)
        context_layer2 = torch.matmul(attention_probs2, value_layer2)

        # 变换成原来的维度以及对应的含义
        context_layer2 = context_layer2.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape2 = context_layer2.size()[:-2] + (self.all_head_size,)
        context_layer2 = context_layer2.view(new_context_layer_shape2)
        attn_data = {
            "attn1": attention_probs1,
            "queries1": query_layer2,
            "keys1": key_layer1,
            "attn2": attention_probs2,
            "querues2": query_layer1,
            "keys2": key_layer2,
        }
        # context_layer1, context_layer2 is for text and vision
        return context_layer1, context_layer2, attn_data



if __name__ == "__main__":
    config = Config()
    biattention = BertBiAttention(config)
    input_tensor1 = torch.rand(1, 10, 768)
    input_tensor2 = torch.rand(1, 10, 768)
    context_layer1, context_layer2, attn_data = biattention(
        input_tensor1, input_tensor2
    )
    print(context_layer1, context_layer1.shape)
```


```text
tensor([[[-0.5275,  0.1879,  0.0525,  ...,  0.2529, -0.3713,  0.6296],
         [-0.5113,  0.1734,  0.0455,  ...,  0.3651, -0.3782,  0.6302],
         [-0.7013,  0.1657,  0.0301,  ...,  0.2873, -0.4464,  0.5825],
         ...,
         [-0.7013,  0.1657,  0.0301,  ...,  0.2873, -0.4464,  0.5825],
         [-0.5713,  0.0028,  0.0240,  ...,  0.3728, -0.5289,  0.7575],
         [-0.5286,  0.1837,  0.0533,  ...,  0.2852, -0.3516,  0.3580]]],
       grad_fn=<ViewBackward>) torch.Size([1, 10, 768])
tensor([[[-0.9755,  0.8821, -1.2967,  ..., -0.3100,  0.4199, -0.7134],
         [-0.8493,  0.7693, -1.0389,  ..., -0.2680,  0.4941, -1.0288],
         [-0.7703,  0.6142, -1.0437,  ..., -0.3502,  0.6243, -1.2214],
         ...,
         [-0.7703,  0.6142, -1.0437,  ..., -0.3502,  0.6243, -1.2214],
         [-0.8464,  0.7717, -1.0359,  ..., -0.1302,  0.1846, -0.4503],
         [-0.9745,  0.8796, -1.2947,  ..., -0.3544,  0.6267, -1.2176]]],
       grad_fn=<ViewBackward>) torch.Size([1, 10, 768])
```



* 当前步骤总结：
	* 通过这一步，我们完成了针对co_attention基于分块思想优化方案，我们可以替换原来co_attention.py进行尝试。




---

#### Step3: 稀疏化思想优化方案 

* 当前步骤简述：
	* 基于稀疏化思想进行attention效率提升的方案设计。


* 方法简述：
	* 原理：
		* 稀疏化是更加灵活的方法，我们以一定的方式将输入的Q/K进行稀疏化，来忽略部分信息以简化计算。
		* 我们可以在chunk的基础上来实现稀疏化，在分割完chunk之后，随机在每个chunk中选择一个张量代表chunk张量，而其他张量都稀疏掉。
	* 实现： 
		* 我们使用torch中的chunk操作来进行


* 代码实现位置：
	* /data/labeled_project/multimodal_labeled/model_train/co_attention_optimization.py



#### 让我们动手做起来吧！


* 代码实现：

```python
import torch
import torch.nn as nn

def sparsification(input_, chunks=2):
    k = torch.chunk(input_, chunks, dim=-2)
    r = []
    for h in k:
        mask = torch.zeros(h.shape)
        mask[:, :, 1, :] = torch.tensor([1]*mask.size()[-1])
        r.append(torch.mul(h, mask))
    return torch.cat(r, dim=-2)

```

> * 运行示例：

```python

if __name__ == "__main__":
    input = torch.rand(1, 12, 10, 43)
    chunks = 2
    w = sparsification(input, chunks)
    print(w.shape)
```

```text
torch.Size([1, 12, 10, 43])
```


* 应用代码实现：
	* 位置：/data/labeled_project/multimodal_labeled/model_train/co_attention_sparse.py

```python
import torch
import torch.nn as nn
import math
from co_attention_optimization import sparsification

class Config:
    def __init__(self):
        super().__init__()
        self.bi_num_attention_heads = 12
        self.bi_hidden_size = 768
        self.v_hidden_size = 768
        self.hidden_size = 768
        self.v_attention_probs_dropout_prob = 0.2
        self.attention_probs_dropout_prob = 0.2


# 互自注意力子层
class BertBiAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        # 多头注意力机制中多头的数量
        self.num_attention_heads = config.bi_num_attention_heads

        # 每个attention头输出的张量最后一维的尺寸
        # 其中config.bi_hidden_size是指希望通过bi-attention之后输出的张量最后一维的尺寸
        # 因为最后要做“拼接”操作，因此每个attention头输出的张量尺寸为二者的商
        self.attention_head_size = int(
            config.bi_hidden_size / config.bi_num_attention_heads
        )

        # 非特殊情况下self.all_head_size与config.bi_hidden_size是相同的
        # 不过二者应用的含义不同，self.all_head_size是指QKV全连接层的输出维度
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        # 视觉Q/K/V参数矩阵
        # 注：按照自注意力机制的原理，一般QKV参数矩阵都是方阵，
        # 即config.v_hidden_size = self.all_head_size
        self.query1 = nn.Linear(config.v_hidden_size, self.all_head_size)
        self.key1 = nn.Linear(config.v_hidden_size, self.all_head_size)
        self.value1 = nn.Linear(config.v_hidden_size, self.all_head_size)
        self.dropout1 = nn.Dropout(config.v_attention_probs_dropout_prob)

        # 文本Q/K/V参数矩阵
        self.query2 = nn.Linear(config.hidden_size, self.all_head_size)
        self.key2 = nn.Linear(config.hidden_size, self.all_head_size)
        self.value2 = nn.Linear(config.hidden_size, self.all_head_size)
        self.dropout2 = nn.Dropout(config.attention_probs_dropout_prob)

    def transpose_for_scores(self, x):
        """在进入attention计算公式前需要做一些准备
        首先需要view，将QKV全连接输出的Q／K／V按头分割
        然后对第二维和第三维进行转置操作，
        为了让代表句子长度维度和词向量维度能够相邻，这样注意力机制才能找到词义与句子位置的关系，
        从attention函数中可以看到，利用的是原始输入的倒数第一和第二维.这样我们就得到了每个头的输入.
        """

        # x的最后一维变成两维，需保证最后一维的size = self.num_attention_heads * self.attention_head_size
        new_x_shape = x.size()[:-1] + (
            self.num_attention_heads,
            self.attention_head_size,
        )
        # 这样输入的三维张量变成了四维，从文本角度，第二为代表长度应该与最后一维的词向量相邻
        x = x.view(new_x_shape)

        # 因此在这里将文本长度维度与attention头数维度调换
        return x.permute(0, 2, 1, 3)

    def forward(self, input_tensor1, input_tensor2):
        # 对视觉输入计算向量
        mixed_query_layer1 = self.query1(input_tensor1)
        mixed_key_layer1 = self.key1(input_tensor1)
        mixed_value_layer1 = self.value1(input_tensor1)
        query_layer1 = self.transpose_for_scores(mixed_query_layer1)
        key_layer1 = self.transpose_for_scores(mixed_key_layer1)
        value_layer1 = self.transpose_for_scores(mixed_value_layer1)
        # 对文本输入计算向量
        mixed_query_layer2 = self.query2(input_tensor2)
        mixed_key_layer2 = self.key2(input_tensor2)
        mixed_value_layer2 = self.value2(input_tensor2)
        query_layer2 = self.transpose_for_scores(mixed_query_layer2)
        key_layer2 = self.transpose_for_scores(mixed_key_layer2)
        value_layer2 = self.transpose_for_scores(mixed_value_layer2)


        query_layer1 = sparsification(query_layer1)
        key_layer1 = sparsification(key_layer1)
        query_layer2 = sparsification(query_layer2)
        key_layer2 = sparsification(key_layer2)

        # attention scores for value 1. 这是关键部分，主要为计算text query 和 image 的 key的结果
        # 视觉的query 与文本的Key的转置进行交叉
        attention_scores1 = torch.matmul(query_layer2, key_layer1.transpose(-1, -2))
        attention_scores1 = attention_scores1 / math.sqrt(self.attention_head_size)
        # 通过softmax 归一化得分概率
        attention_probs1 = nn.functional.softmax(attention_scores1, dim=-1)
        attention_probs1 = self.dropout1(attention_probs1)
        # 得到的归一化概率与文本value进行运算得到context_layer
        context_layer1 = torch.matmul(attention_probs1, value_layer1)

        # 变换成原来的维度以及对应的含义
        context_layer1 = context_layer1.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape1 = context_layer1.size()[:-2] + (self.all_head_size,)
        context_layer1 = context_layer1.view(new_context_layer_shape1)
        # 文本的query与视觉的key 进行运算，运算方式与上面相同
        attention_scores2 = torch.matmul(query_layer1, key_layer2.transpose(-1, -2))
        attention_scores2 = attention_scores2 / math.sqrt(self.attention_head_size)
        # Normalize
        attention_probs2 = nn.functional.softmax(attention_scores2, dim=-1)
        attention_probs2 = self.dropout2(attention_probs2)
        context_layer2 = torch.matmul(attention_probs2, value_layer2)

        # 变换成原来的维度以及对应的含义
        context_layer2 = context_layer2.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape2 = context_layer2.size()[:-2] + (self.all_head_size,)
        context_layer2 = context_layer2.view(new_context_layer_shape2)
        attn_data = {
            "attn1": attention_probs1,
            "queries1": query_layer2,
            "keys1": key_layer1,
            "attn2": attention_probs2,
            "querues2": query_layer1,
            "keys2": key_layer2,
        }
        # context_layer1, context_layer2 is for text and vision
        return context_layer1, context_layer2, attn_data





if __name__ == "__main__":
    config = Config()
    biattention = BertBiAttention(config)
    input_tensor1 = torch.rand(1, 10, 768)
    input_tensor2 = torch.rand(1, 10, 768)
    context_layer1, context_layer2, attn_data = biattention(
        input_tensor1, input_tensor2
    )
    print(context_layer1, context_layer1.shape)
```


```text
tensor([[[-0.4802,  0.0567,  0.0621,  ...,  0.0756, -0.1372,  0.5268],
         [-0.4696,  0.0685,  0.0563,  ...,  0.1154, -0.1814,  0.5222],
         [-0.3768,  0.0255,  0.0857,  ...,  0.0665, -0.2988,  0.5007],
         ...,
         [-0.2812,  0.0131,  0.0403,  ...,  0.0854, -0.1733,  0.5327],
         [-0.3351,  0.0453,  0.0286,  ...,  0.0899, -0.2571,  0.6467],
         [-0.4802,  0.0567,  0.0621,  ...,  0.0983, -0.0754,  0.4280]]],
       grad_fn=<ViewBackward>) torch.Size([1, 10, 768])
tensor([[[-0.1374,  0.4510, -0.1282,  ..., -0.3784,  0.1646, -0.2787],
         [-0.1527,  0.5110, -0.1016,  ..., -0.3849,  0.1610, -0.2874],
         [-0.1426,  0.4916, -0.1059,  ..., -0.3504,  0.1462, -0.1991],
         ...,
         [-0.0654,  0.3229, -0.0535,  ..., -0.3073,  0.1559, -0.1695],
         [-0.1334,  0.4861, -0.1000,  ..., -0.2750,  0.1237, -0.1738],
         [-0.1227,  0.3599, -0.0613,  ..., -0.2665,  0.1496, -0.1444]]],
       grad_fn=<ViewBackward>) torch.Size([1, 10, 768])
```



* 一般情况下chunk或稀疏化对attention的优化效果：


![](http://www.tisv.cn:8900/img/attention_res.png)



* 当前步骤总结：
	* 通过这一步，我们完成了针对co_attention基于稀疏化思想优化方案，我们可以替换原来co_attention.py进行尝试。




---

